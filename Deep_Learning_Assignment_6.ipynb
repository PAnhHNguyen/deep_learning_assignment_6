{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtuSrazlNYEL"
      },
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vriXNd_nL2q6"
      },
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OdOOqDBI-iw-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5atve1sMH9n"
      },
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QvKdt5EyMDug"
      },
      "outputs": [],
      "source": [
        "# Download 'The Tale of Peter Rabbit'\n",
        "url = \"https://www.gutenberg.org/cache/epub/28885/pg28885.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Remove Project Gutenberg header and footer\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\"\n",
        "\n",
        "if start_marker in text and end_marker in text:\n",
        "    content = text.split(start_marker)[1].split(end_marker)[0].strip()\n",
        "else:\n",
        "    raise ValueError(\"Start or end marker not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      },
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RvXRFVcbMLe9"
      },
      "outputs": [],
      "source": [
        "# Clean text: lowercase and remove all punctuation except .,!,?\n",
        "punct_to_remove = string.punctuation.replace('.', '').replace('!', '').replace('?', '')\n",
        "table = str.maketrans('', '', punct_to_remove)\n",
        "cleaned_text = content.lower().translate(table)\n",
        "\n",
        "# Word-level tokenization\n",
        "tokens = cleaned_text.split()\n",
        "total_words = len(tokens)\n",
        "split_index = int(0.8 * total_words)\n",
        "train_tokens = tokens[:split_index]\n",
        "val_tokens = tokens[split_index:]\n",
        "\n",
        "# Build vocab\n",
        "word_counts = Counter(train_tokens)\n",
        "min_word_freq = 5\n",
        "vocab = [\"<unk>\"] + [word for word, count in word_counts.items() if count >= min_word_freq]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "unk_id = word_to_id[\"<unk>\"]\n",
        "\n",
        "# Encode\n",
        "train_ids = [word_to_id.get(word, unk_id) for word in train_tokens]\n",
        "val_ids = [word_to_id.get(word, unk_id) for word in val_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbTZs3OiMMNy"
      },
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bXSQxlhuAzV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a60661a-9fa3-4c03-ac62-a1ac91746334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define the embedding layer\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=len(word_to_id),\n",
        "    output_dim=128,\n",
        "    input_length=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsXR4RZpMXMi"
      },
      "source": [
        "## 4. Model & 5. Training and Evaluation\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P8d8FS2XMj46"
      },
      "outputs": [],
      "source": [
        "class PerplexityCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_loss = logs.get(\"loss\")\n",
        "        val_loss = logs.get(\"val_loss\")\n",
        "\n",
        "        train_ppl = math.exp(train_loss) if train_loss else float(\"inf\")\n",
        "        val_ppl = math.exp(val_loss) if val_loss else float(\"inf\")\n",
        "\n",
        "        print(f\"\\n Epoch {epoch}:\")\n",
        "        print(f\"   - Training   Loss: {train_loss:.4f} | Perplexity: {train_ppl:.2f}\")\n",
        "        print(f\"   - Validation Loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "linweGaUMg0T"
      },
      "outputs": [],
      "source": [
        "sequence_length = 10\n",
        "\n",
        "def create_sequences(ids, seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_len, len(ids)):\n",
        "        X.append(ids[i - seq_len:i])\n",
        "        y.append(ids[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X_train, y_train = create_sequences(train_ids, sequence_length)\n",
        "X_val, y_val = create_sequences(val_ids, sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hNhdzNSYWwL",
        "outputId": "07d9a367-aac8-4722-f39c-3e87c1f9ddb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.1559 - loss: 6.0153\n",
            " Epoch 0:\n",
            "   - Training   Loss: 5.5115 | Perplexity: 247.53\n",
            "   - Validation Loss: 4.6357 | Perplexity: 103.10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 154ms/step - accuracy: 0.1560 - loss: 6.0139 - val_accuracy: 0.2321 - val_loss: 4.6357 - learning_rate: 5.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.1815 - loss: 4.8944\n",
            " Epoch 1:\n",
            "   - Training   Loss: 4.8863 | Perplexity: 132.46\n",
            "   - Validation Loss: 4.5738 | Perplexity: 96.91\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 152ms/step - accuracy: 0.1815 - loss: 4.8944 - val_accuracy: 0.2321 - val_loss: 4.5738 - learning_rate: 5.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.1819 - loss: 4.8494\n",
            " Epoch 2:\n",
            "   - Training   Loss: 4.8599 | Perplexity: 129.01\n",
            "   - Validation Loss: 4.5555 | Perplexity: 95.15\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 146ms/step - accuracy: 0.1819 - loss: 4.8495 - val_accuracy: 0.2321 - val_loss: 4.5555 - learning_rate: 5.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.1764 - loss: 4.8683\n",
            " Epoch 3:\n",
            "   - Training   Loss: 4.8566 | Perplexity: 128.58\n",
            "   - Validation Loss: 4.5459 | Perplexity: 94.25\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 154ms/step - accuracy: 0.1764 - loss: 4.8683 - val_accuracy: 0.2321 - val_loss: 4.5459 - learning_rate: 5.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.1815 - loss: 4.8429\n",
            " Epoch 4:\n",
            "   - Training   Loss: 4.8514 | Perplexity: 127.92\n",
            "   - Validation Loss: 4.5450 | Perplexity: 94.16\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 149ms/step - accuracy: 0.1815 - loss: 4.8429 - val_accuracy: 0.2321 - val_loss: 4.5450 - learning_rate: 5.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.1811 - loss: 4.8361\n",
            " Epoch 5:\n",
            "   - Training   Loss: 4.8473 | Perplexity: 127.39\n",
            "   - Validation Loss: 4.5482 | Perplexity: 94.46\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 156ms/step - accuracy: 0.1811 - loss: 4.8362 - val_accuracy: 0.2321 - val_loss: 4.5482 - learning_rate: 5.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.1758 - loss: 4.8578\n",
            " Epoch 6:\n",
            "   - Training   Loss: 4.8451 | Perplexity: 127.11\n",
            "   - Validation Loss: 4.5397 | Perplexity: 93.67\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 155ms/step - accuracy: 0.1758 - loss: 4.8577 - val_accuracy: 0.2321 - val_loss: 4.5397 - learning_rate: 5.0000e-05\n",
            "Epoch 8/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.1764 - loss: 4.8610\n",
            " Epoch 7:\n",
            "   - Training   Loss: 4.8426 | Perplexity: 126.80\n",
            "   - Validation Loss: 4.5348 | Perplexity: 93.20\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 176ms/step - accuracy: 0.1764 - loss: 4.8609 - val_accuracy: 0.2321 - val_loss: 4.5348 - learning_rate: 5.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.1791 - loss: 4.8371\n",
            " Epoch 8:\n",
            "   - Training   Loss: 4.8414 | Perplexity: 126.65\n",
            "   - Validation Loss: 4.5304 | Perplexity: 92.80\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 150ms/step - accuracy: 0.1791 - loss: 4.8371 - val_accuracy: 0.2323 - val_loss: 4.5304 - learning_rate: 5.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.1818 - loss: 4.8293\n",
            " Epoch 9:\n",
            "   - Training   Loss: 4.8391 | Perplexity: 126.35\n",
            "   - Validation Loss: 4.5264 | Perplexity: 92.42\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 148ms/step - accuracy: 0.1818 - loss: 4.8293 - val_accuracy: 0.2323 - val_loss: 4.5264 - learning_rate: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e48b2d14f50>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Embedding(vocab_size, 128, input_length=sequence_length, mask_zero=True),\n",
        "    LSTM(256, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(256),\n",
        "    Dropout(0.2),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer = AdamW(learning_rate=5e-5, weight_decay=1e-6),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2),\n",
        "    PerplexityCallback()\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbvbBOp3MfTD"
      },
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1uHjn6aHMW5K"
      },
      "outputs": [],
      "source": [
        "def generate_text(seed_text, num_tokens=50):\n",
        "    result = seed_text.lower().split()\n",
        "    for _ in range(num_tokens):\n",
        "        # Encode and pad the current input\n",
        "        encoded = [word_to_id.get(word, unk_id) for word in result[-sequence_length:]]\n",
        "        padded = tf.keras.preprocessing.sequence.pad_sequences([encoded], maxlen=sequence_length)\n",
        "\n",
        "        # Predict next word\n",
        "        pred_probs = model.predict(padded, verbose=0)[0]\n",
        "        next_id = np.random.choice(len(pred_probs), p=pred_probs)\n",
        "        next_word = id_to_word.get(next_id, \"<unk>\")\n",
        "\n",
        "        result.append(next_word)\n",
        "    return ' '.join(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed1 = \"she said\"\n",
        "sample1 = generate_text(seed1, num_tokens=50)\n",
        "print(\"Sample 1:\")\n",
        "print(sample1)\n",
        "\n",
        "seed2 = \"then they\"\n",
        "sample2 = generate_text(seed2, num_tokens=50)\n",
        "print(\"\\nSample 2:\")\n",
        "print(sample2)"
      ],
      "metadata": {
        "id": "6FQtfODE9rqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14caf306-acfa-4dbd-9778-0488edc57426"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1:\n",
            "she said it! <unk> indeed the theres he its be poor was to <unk> believe window to seen come <unk> of <unk> <unk> whether see <unk> to his <unk> <unk> only father i <unk> a <unk> either make and the she eyes <unk> a do one far minute her tale <unk> the\n",
            "\n",
            "Sample 2:\n",
            "then they though world with though <unk> asking an cat to <unk> in do and that <unk> took if down found say <unk> cried growing that king <unk> alice and my of <unk> said more <unk> a had you so it do herself mad. in that to and rather duchess the <unk>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}